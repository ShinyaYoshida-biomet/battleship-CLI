{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d9b8c6f18014aae83db9b60b854125e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b14b46ed71df4f5381facf45667e948d",
              "IPY_MODEL_2423935283e849f3a8b1eddae3b1420e",
              "IPY_MODEL_b325b179aae144e9b1644f95fa33788f"
            ],
            "layout": "IPY_MODEL_2c66f8d9b2d5480293d9a4279cac1a1b"
          }
        },
        "b14b46ed71df4f5381facf45667e948d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a93e3e012e4e10b44d9f568df632d7",
            "placeholder": "​",
            "style": "IPY_MODEL_83e4ad35b2aa4ad7bdd7aaf5cd0b3f6f",
            "value": "Map: 100%"
          }
        },
        "2423935283e849f3a8b1eddae3b1420e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb70121cf4034621959fa1c2765a0497",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9f2f9de733744eeb834c7e8bef25ffc",
            "value": 20
          }
        },
        "b325b179aae144e9b1644f95fa33788f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aca7c0546554c3a90bca1e8d672a42b",
            "placeholder": "​",
            "style": "IPY_MODEL_a9a573b2bb694a279023cebed7e47ce2",
            "value": " 20/20 [00:00&lt;00:00, 614.43 examples/s]"
          }
        },
        "2c66f8d9b2d5480293d9a4279cac1a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a93e3e012e4e10b44d9f568df632d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83e4ad35b2aa4ad7bdd7aaf5cd0b3f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb70121cf4034621959fa1c2765a0497": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f2f9de733744eeb834c7e8bef25ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8aca7c0546554c3a90bca1e8d672a42b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a573b2bb694a279023cebed7e47ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df840597-64ce-4834-852e-48ced451f69f"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-ai-edge/mediapipe-samples/blob/main/codelabs/litert_inference/gemma3_1b_tflite.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install dependencies"
      ],
      "metadata": {
        "id": "39AMoCOa1ckc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install faiss-cpu\n",
        "! pip install datasets\n",
        "! pip install ai_edge_torch\n",
        "! pip install ai-edge-litert\n",
        "! pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
      ],
      "metadata": {
        "id": "43tAeO0AZ7zp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ai_edge_litert import interpreter as interpreter_lib\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from collections.abc import Sequence\n",
        "import sys"
      ],
      "metadata": {
        "id": "i6PMkMVBPr1p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prerequisite"
      ],
      "metadata": {
        "id": "qgwkcOcjGKEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create HuggingFace token with permission access to\n",
        "\n",
        "  - litert-community/Gemma3-1B-IT\n",
        "\n",
        "  - google/gemma-3-1b-it\n",
        "\n",
        "  This is needed to download the tflite model and tokenizer.\n",
        "\n",
        "- Open Colab Secrets: In your Google Colab notebook, locate the Secrets icon in the left-hand sidebar and click on it.\n",
        "- Add a new secret: Click the \"Add Secret\" button.\n",
        "- Name your secret: Enter \"HF_TOKEN\" for your token in the \"Name\" field.\n",
        "- Paste your token: In the \"Value\" field, paste the actual token you want to store."
      ],
      "metadata": {
        "id": "868qAg3KGNVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download model files"
      ],
      "metadata": {
        "id": "f3M1Og3FQEAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(repo_id=\"litert-community/Gemma3-1B-IT\", filename=\"gemma3-1b-it-int4.tflite\")"
      ],
      "metadata": {
        "id": "N1YlSDEKQG9o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create LiteRT interpreter and tokenizer"
      ],
      "metadata": {
        "id": "n5Xa4s6XhWqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = interpreter_lib.InterpreterWithCustomOps(\n",
        "    custom_op_registerers=[\"pywrap_genai_ops.GenAIOpsRegisterer\"],\n",
        "    model_path=model_path,\n",
        "    num_threads=2,\n",
        "    experimental_default_delegate_latest_features=True)"
      ],
      "metadata": {
        "id": "Rvdn3EIZhaQn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = 'google/gemma-3-1b-it'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "H-wNgbK-aMGh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create pipeline with LiteRT models"
      ],
      "metadata": {
        "id": "AM6rDABTXt2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_mask(shape: Sequence[int], k: int):\n",
        "  \"\"\"Gets the mask for the input to the model.\n",
        "\n",
        "  Args:\n",
        "    shape: The shape of the mask input to the model.\n",
        "    k: all elements below the k-th diagonal are set to 0.\n",
        "\n",
        "  Returns:\n",
        "    The mask for the input to the model. All the elements in the mask are set\n",
        "    to -inf except that all the elements below the k-th diagonal are set to 0.\n",
        "  \"\"\"\n",
        "  mask = np.ones(shape, dtype=np.float32) * float(\"-inf\")\n",
        "  mask = np.triu(mask, k=k)\n",
        "  return mask\n",
        "\n",
        "class LiteRTLlmPipeline:\n",
        "\n",
        "  def __init__(self, interpreter, tokenizer):\n",
        "    \"\"\"Initializes the pipeline.\"\"\"\n",
        "    self._interpreter = interpreter\n",
        "    self._tokenizer = tokenizer\n",
        "\n",
        "    self._prefill_runner = None\n",
        "    self._decode_runner = self._interpreter.get_signature_runner(\"decode\")\n",
        "\n",
        "\n",
        "  def _init_prefill_runner(self, num_input_tokens: int):\n",
        "    \"\"\"Initializes all the variables related to the prefill runner.\n",
        "\n",
        "    This method initializes the following variables:\n",
        "      - self._prefill_runner: The prefill runner based on the input size.\n",
        "      - self._max_seq_len: The maximum sequence length supported by the model.\n",
        "\n",
        "    Args:\n",
        "      num_input_tokens: The number of input tokens.\n",
        "    \"\"\"\n",
        "    if not self._interpreter:\n",
        "      raise ValueError(\"Interpreter is not initialized.\")\n",
        "\n",
        "    # Prefill runner related variables will be initialized in `predict_text` and\n",
        "    # `compute_log_likelihood`.\n",
        "    self._prefill_runner = self._get_prefill_runner(num_input_tokens)\n",
        "    # input_token_shape has shape (batch, max_seq_len)\n",
        "    input_token_shape = self._prefill_runner.get_input_details()[\"tokens\"][\n",
        "        \"shape\"\n",
        "    ]\n",
        "    if len(input_token_shape) == 1:\n",
        "      self._max_seq_len = input_token_shape[0]\n",
        "    else:\n",
        "      self._max_seq_len = input_token_shape[1]\n",
        "\n",
        "    # kv cache input has shape [batch=1, num_kv_heads, cache_size, head_dim].\n",
        "    kv_cache_shape = self._prefill_runner.get_input_details()[\"kv_cache_k_0\"][\n",
        "        \"shape\"\n",
        "    ]\n",
        "    self._max_kv_cache_seq_len = kv_cache_shape[2]\n",
        "\n",
        "  def _init_kv_cache(self) -> dict[str, np.ndarray]:\n",
        "    if self._prefill_runner is None:\n",
        "      raise ValueError(\"Prefill runner is not initialized.\")\n",
        "    kv_cache = {}\n",
        "    for input_key in self._prefill_runner.get_input_details().keys():\n",
        "      if \"kv_cache\" in input_key:\n",
        "        kv_cache[input_key] = np.zeros(\n",
        "            self._prefill_runner.get_input_details()[input_key][\"shape\"],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        kv_cache[input_key] = np.zeros(\n",
        "            self._prefill_runner.get_input_details()[input_key][\"shape\"],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "    return kv_cache\n",
        "\n",
        "  def _get_prefill_runner(self, num_input_tokens: int) :\n",
        "    \"\"\"Gets the prefill runner with the best suitable input size.\n",
        "\n",
        "    Args:\n",
        "      num_input_tokens: The number of input tokens.\n",
        "\n",
        "    Returns:\n",
        "      The prefill runner with the smallest input size.\n",
        "    \"\"\"\n",
        "    best_signature = None\n",
        "    delta = sys.maxsize\n",
        "    max_prefill_len = -1\n",
        "    for key in self._interpreter.get_signature_list().keys():\n",
        "      if \"prefill\" not in key:\n",
        "        continue\n",
        "      input_pos = self._interpreter.get_signature_runner(key).get_input_details()[\n",
        "          \"input_pos\"\n",
        "      ]\n",
        "      # input_pos[\"shape\"] has shape (max_seq_len, )\n",
        "      seq_size = input_pos[\"shape\"][0]\n",
        "      max_prefill_len = max(max_prefill_len, seq_size)\n",
        "      if num_input_tokens <= seq_size and seq_size - num_input_tokens < delta:\n",
        "        delta = seq_size - num_input_tokens\n",
        "        best_signature = key\n",
        "    if best_signature is None:\n",
        "      raise ValueError(\n",
        "          \"The largest prefill length supported is %d, but we have %d number of input tokens\"\n",
        "          %(max_prefill_len, num_input_tokens)\n",
        "      )\n",
        "    return self._interpreter.get_signature_runner(best_signature)\n",
        "\n",
        "  def _run_prefill(\n",
        "      self, prefill_token_ids: Sequence[int],\n",
        "  ) -> dict[str, np.ndarray]:\n",
        "    \"\"\"Runs prefill and returns the kv cache.\n",
        "\n",
        "    Args:\n",
        "      prefill_token_ids: The token ids of the prefill input.\n",
        "\n",
        "    Returns:\n",
        "      The updated kv cache.\n",
        "    \"\"\"\n",
        "    if not self._prefill_runner:\n",
        "      raise ValueError(\"Prefill runner is not initialized.\")\n",
        "    prefill_token_length = len(prefill_token_ids)\n",
        "    if prefill_token_length == 0:\n",
        "      return self._init_kv_cache()\n",
        "\n",
        "    # Prepare the input to be [1, max_seq_len].\n",
        "    input_token_ids = [0] * self._max_seq_len\n",
        "    input_token_ids[:prefill_token_length] = prefill_token_ids\n",
        "    input_token_ids = np.asarray(input_token_ids, dtype=np.int32)\n",
        "    input_token_ids = np.expand_dims(input_token_ids, axis=0)\n",
        "\n",
        "    # Prepare the input position to be [max_seq_len].\n",
        "    input_pos = [0] * self._max_seq_len\n",
        "    input_pos[:prefill_token_length] = range(prefill_token_length)\n",
        "    input_pos = np.asarray(input_pos, dtype=np.int32)\n",
        "\n",
        "    # Initialize kv cache.\n",
        "    prefill_inputs = self._init_kv_cache()\n",
        "    # Prepare the tokens and input position inputs.\n",
        "    prefill_inputs.update({\n",
        "        \"tokens\": input_token_ids,\n",
        "        \"input_pos\": input_pos,\n",
        "    })\n",
        "    if \"mask\" in self._prefill_runner.get_input_details().keys():\n",
        "      # For prefill, mask has shape [batch=1, 1, seq_len, kv_cache_size].\n",
        "      # We want mask[0, 0, i, j] = 0 for j<=i and -inf otherwise.\n",
        "      prefill_inputs[\"mask\"] = _get_mask(\n",
        "          shape=self._prefill_runner.get_input_details()[\"mask\"][\"shape\"],\n",
        "          k=1,\n",
        "      )\n",
        "    prefill_outputs = self._prefill_runner(**prefill_inputs)\n",
        "    if \"logits\" in prefill_outputs:\n",
        "      # Prefill outputs includes logits and kv cache. We only output kv cache.\n",
        "      prefill_outputs.pop(\"logits\")\n",
        "\n",
        "    return prefill_outputs\n",
        "\n",
        "  def _greedy_sampler(self, logits: np.ndarray) -> int:\n",
        "    return int(np.argmax(logits))\n",
        "\n",
        "\n",
        "  def _run_decode(\n",
        "      self,\n",
        "      start_pos: int,\n",
        "      start_token_id: int,\n",
        "      kv_cache: dict[str, np.ndarray],\n",
        "      max_decode_steps: int,\n",
        "  ) -> str:\n",
        "    \"\"\"Runs decode and outputs the token ids from greedy sampler.\n",
        "\n",
        "    Args:\n",
        "      start_pos: The position of the first token of the decode input.\n",
        "      start_token_id: The token id of the first token of the decode input.\n",
        "      kv_cache: The kv cache from the prefill.\n",
        "      max_decode_steps: The max decode steps.\n",
        "\n",
        "    Returns:\n",
        "      The token ids from the greedy sampler.\n",
        "    \"\"\"\n",
        "    next_pos = start_pos\n",
        "    next_token = start_token_id\n",
        "    decode_text = []\n",
        "    decode_inputs = kv_cache\n",
        "\n",
        "    for _ in range(max_decode_steps):\n",
        "      decode_inputs.update({\n",
        "          \"tokens\": np.array([[next_token]], dtype=np.int32),\n",
        "          \"input_pos\": np.array([next_pos], dtype=np.int32),\n",
        "      })\n",
        "      if \"mask\" in self._decode_runner.get_input_details().keys():\n",
        "        # For decode, mask has shape [batch=1, 1, 1, kv_cache_size].\n",
        "        # We want mask[0, 0, 0, j] = 0 for j<=next_pos and -inf otherwise.\n",
        "        decode_inputs[\"mask\"] = _get_mask(\n",
        "            shape=self._decode_runner.get_input_details()[\"mask\"][\"shape\"],\n",
        "            k=next_pos + 1,\n",
        "        )\n",
        "      decode_outputs = self._decode_runner(**decode_inputs)\n",
        "      # Output logits has shape (batch=1, 1, vocab_size). We only take the first\n",
        "      # element.\n",
        "      logits = decode_outputs.pop(\"logits\")[0][0]\n",
        "      next_token = self._greedy_sampler(logits)\n",
        "      if next_token == self._tokenizer.eos_token_id:\n",
        "        break\n",
        "      decode_text.append(self._tokenizer.decode(next_token, skip_special_tokens=True))\n",
        "      if len(decode_text[-1]) == 0:\n",
        "        # Break out the loop if we hit the special token.\n",
        "        break\n",
        "\n",
        "      print(decode_text[-1], end='', flush=True)\n",
        "      # Decode outputs includes logits and kv cache. We already poped out\n",
        "      # logits, so the rest is kv cache. We pass the updated kv cache as input\n",
        "      # to the next decode step.\n",
        "      decode_inputs = decode_outputs\n",
        "      next_pos += 1\n",
        "\n",
        "    print() # print a new line at the end.\n",
        "    return ''.join(decode_text)\n",
        "\n",
        "  def generate(self, prompt: str, max_decode_steps: int | None = None) -> str:\n",
        "    messages=[{\"role\": \"system\", \"content\": \"You are a sales person\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    token_ids = self._tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
        "    # Initialize the prefill runner with the suitable input size.\n",
        "    self._init_prefill_runner(len(token_ids))\n",
        "\n",
        "    # Run prefill.\n",
        "    # Prefill up to the seond to the last token of the prompt, because the last\n",
        "    # token of the prompt will be used to bootstrap decode.\n",
        "    prefill_token_length = len(token_ids) - 1\n",
        "\n",
        "    print('Running prefill')\n",
        "    kv_cache = self._run_prefill(token_ids[:prefill_token_length])\n",
        "    # Run decode.\n",
        "    print('Running decode')\n",
        "    actual_max_decode_steps = self._max_kv_cache_seq_len - prefill_token_length - 1\n",
        "    if max_decode_steps is not None:\n",
        "      actual_max_decode_steps = min(actual_max_decode_steps, max_decode_steps)\n",
        "    decode_text = self._run_decode(\n",
        "        prefill_token_length,\n",
        "        token_ids[prefill_token_length],\n",
        "        kv_cache,\n",
        "        actual_max_decode_steps,\n",
        "    )\n",
        "    return decode_text"
      ],
      "metadata": {
        "id": "UBSGrHrM4ANm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate text from model"
      ],
      "metadata": {
        "id": "dASKx_JtYXwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disclaimer: Model performance demonstrated with the Python API in this notebook is not representative of performance on a local device.\n",
        "pipeline = LiteRTLlmPipeline(interpreter, tokenizer)"
      ],
      "metadata": {
        "id": "AZhlDQWg61AL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# ------------- STEP 1: Create a simple Retriever (indexing documents) -------------\n",
        "class SimpleRetriever:\n",
        "    def __init__(self, documents: List[str], embedding_model):\n",
        "        self.documents = documents\n",
        "        self.embedding_model = embedding_model\n",
        "        self.index = None\n",
        "        self.embeddings = None\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        self.embeddings = np.array([self._embed(doc) for doc in self.documents]).astype('float32')\n",
        "        self.index = faiss.IndexFlatL2(self.embeddings.shape[1])\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def _embed(self, text: str) -> np.ndarray:\n",
        "        inputs = self.embedding_model(text, max_decode_steps=5)  # Light hack: just for simple embedding\n",
        "        return np.random.rand(384)  # 👉 Replace this with *real* embeddings later\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        query_emb = np.expand_dims(self._embed(query), axis=0).astype('float32')\n",
        "        _, indices = self.index.search(query_emb, top_k)\n",
        "        return [self.documents[i] for i in indices[0]]\n",
        "\n",
        "# ------------- STEP 2: Update your pipeline to use retrieved docs -------------\n",
        "class LiteRAGPipeline(LiteRTLlmPipeline):\n",
        "\n",
        "    def __init__(self, interpreter, tokenizer, retriever):\n",
        "        super().__init__(interpreter, tokenizer)\n",
        "        self.retriever = retriever\n",
        "\n",
        "    def generate_with_rag(self, user_query: str, max_decode_steps: int = 200) -> str:\n",
        "        # Step 1: Retrieve relevant docs\n",
        "        retrieved_docs = self.retriever.retrieve(user_query)\n",
        "\n",
        "        # Step 2: Build the context\n",
        "        context = \"\\n\".join(retrieved_docs)\n",
        "\n",
        "        # Step 3: Combine context and user query\n",
        "        prompt = f\"Use the following information to answer the question.\\n\\n{context}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
        "\n",
        "        # Step 4: Run the model's generation\n",
        "        return self.generate(prompt, max_decode_steps=max_decode_steps)\n",
        "\n",
        "# ------------- STEP 3: Put it together! -------------\n",
        "# Load your sales documents\n",
        "documents = [\n",
        "    \"Our coffee beans are sourced from Ethiopia, Colombia, and Brazil.\",\n",
        "    \"We offer discounts on orders over 10kg.\",\n",
        "    \"Our best-selling perfume is 'Rose Blossom', made from organic ingredients.\"\n",
        "]\n",
        "\n",
        "# Assume you have a simple dummy embedding model (replace later)\n",
        "def dummy_embedder(text, max_decode_steps=5):\n",
        "    # 👉 You can replace this with a real sentence embedding model later\n",
        "    return np.random.rand(384)\n",
        "\n",
        "retriever = SimpleRetriever(documents=documents, embedding_model=dummy_embedder)\n",
        "\n",
        "# Instantiate the new RAG-enabled pipeline\n",
        "rag_pipeline = LiteRAGPipeline(interpreter, tokenizer, retriever)\n",
        "\n",
        "# Now you can use:\n",
        "answer = rag_pipeline.generate_with_rag(\"What kind of perfume fits to uni-sex person?\")\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFPFwPQIG7TH",
        "outputId": "2408245c-2cd8-4cb6-ad74-4861adc564a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running prefill\n",
            "Running decode\n",
            "Okay! Let's talk about perfumes that would be perfect for an unisex person – someone who appreciates quality and natural ingredients like 'Rose Blossom' you mentioned!\n",
            "\n",
            "We offer an extensive range of perfumes tailored specifically for those who want something sophisticated and beautifully scented without any gender boundaries.  While 'Rose Blossom' certainly evokes romance and beauty – which can be appealing – we’ve curated collections designed for everyone who wants an elegant and uplifting fragrance experience.\n",
            "\n",
            "We’re currently offering discounts on orders over ten kilograms! So you can have multiple bottles or have your favorite scent on hand without breaking your budget.\n",
            "\n",
            "We’re confident you’ll find something you love!\n",
            "Answer: Okay! Let's talk about perfumes that would be perfect for an unisex person – someone who appreciates quality and natural ingredients like 'Rose Blossom' you mentioned!\n",
            "\n",
            "We offer an extensive range of perfumes tailored specifically for those who want something sophisticated and beautifully scented without any gender boundaries.  While 'Rose Blossom' certainly evokes romance and beauty – which can be appealing – we’ve curated collections designed for everyone who wants an elegant and uplifting fragrance experience.\n",
            "\n",
            "We’re currently offering discounts on orders over ten kilograms! So you can have multiple bottles or have your favorite scent on hand without breaking your budget.\n",
            "\n",
            "We’re confident you’ll find something you love!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hi, I am really hungry, do you have any suggestions to eat?\"\n",
        "print(\"Answer Assistant Q1:\")\n",
        "output = {pipeline.generate(prompt)}\n",
        "print(\"================================================================\")\n",
        "\n",
        "prompt = \"What kind of perfume fits to uni-sex person?\"\n",
        "print(\"Answer Assistant Q2:\")\n",
        "output = {pipeline.generate(prompt)}\n",
        "print(\"================================================================\")\n",
        "\n",
        "prompt = \"When I feel so tired, is it better to drink coffee?\"\n",
        "print(\"Answer Assistant Q3:\")\n",
        "output = {pipeline.generate(prompt)}\n",
        "print(\"================================================================\")\n",
        "\n",
        "prompt = \"Why does the perfume with citrus smell so nice?\"\n",
        "print(\"Answer Assistant Q4:\")\n",
        "output = {pipeline.generate(prompt)}\n",
        "print(\"================================================================\")\n",
        "\n",
        "prompt = \"Do you know somewhere to buy humburger?\"\n",
        "print(\"Answer Assistant Q5:\")\n",
        "output = {pipeline.generate(prompt)}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u7s_sdxDb-h",
        "outputId": "fb4314e7-f388-4803-ff4b-3f0b6f21d58c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer Assistant Q1:\n",
            "Running prefill\n",
            "Running decode\n",
            "Okay! Let's see what can help you satisfy your hunger. I understand you're feeling really hungry! Let's brainstorm some options – I want you feeling good and energized!  I can give you some suggestions based on what’s available right now – but let’s consider what you’re craving!\n",
            "\n",
            "Here are some ideas:\n",
            "\n",
            "*   **Quick & Easy:**\n",
            "    *   **Fruit:** A banana or apple can provide some energy boost!\n",
            "    *   **Nuts:** A handful can provide healthy fats and protein\n",
            "    *   **Yogurt:** It can provide some protein and probiotics\n",
            "*   **Something Slightly More substantial:**\n",
            "    *   **Sandwich:** A simple sandwich can provide some carbs and protein\n",
            "    *   **Granola Bar:** A quick snack that can provide some energy\n",
            "    *   **Leftovers:** What have you had recently? A quick meal can provide energy!\n",
            "\n",
            "**Do you want me give you some more specific suggestions?** For example:\n",
            "\n",
            "*   Do you want something sweet or savory?\n",
            "*   Are you craving something healthy or indulgent?\n",
            "*   Do you want something quick or are you willing wait?\n",
            "================================================================\n",
            "Answer Assistant Q2:\n",
            "Running prefill\n",
            "Running decode\n",
            "Okay! Let's talk about perfumes that are really well-suited for an unisex person – you know you want something sophisticated but approachable! Here’s what I’m thinking:\n",
            "\n",
            "**I’m going with something leaning towards:**\n",
            "\n",
            "*   **Fresh & Fruity:** A scent that’s clean but isn’t overly sweet or heavy. Think notes like citrus (like grapefruit or lemon), berries (like raspberry or blueberry), or something like green tea or mint – something bright but not overpowering.\n",
            "*   **Warm & Slightly Spicy:** A touch of warmth can be really appealing – something like sandalwood or amber; but not too overpowering or masculine.\n",
            "*   **Clean & Natural:** This can be really good – something like rose or jasmine; but make sure there’s an underlying freshness!\n",
            "\n",
            "**Here are some specific recommendations based on different price points:**\n",
            "\n",
            "*   **Mid-Range ($60-80):**\n",
            "    *   **Byredo - This Island:** This one’s an absolute winner! It’s an incredibly well-balanced scent – fresh and aquatic notes with an underlying woody base. It’s versatile enough for everyday wear and feels really elegant.\n",
            "    *   **Nico Is There Something Wrong?:** This scent offers an intriguing blend of citrus and florals – really refreshing and intriguing.\n",
            "*   **Higher End ($80-120):**\n",
            "    *   **Chanel - Chance:** This one’s an absolute classic! It’s an elegant blend of citrus and florals – really sophisticated and versatile.\n",
            "    *   **Dior - All Day:** This scent offers an uplifting blend of citrus and florals – perfect for everyday wear and feels really confident.\n",
            "*   **More Affordable ($60-80):**\n",
            "    *   **Versace - Green Notes:** This provides an incredibly fresh and clean scent – perfect as an everyday scent!\n",
            "    *   **Giorgio Armani - Woman:** This provides an elegant blend of florals and citrus notes!\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Skin Test:** If you’re unsure about something you’re considering buying you should always test your skin first!\n",
            "*   **Personal Preference:** Ultimately, what *you* like will be most important! Don’t be afraid to experiment!\n",
            "\n",
            "**Do you want me give you some more tailored suggestions based on your preferences?** For example:\n",
            "\n",
            "*   Do you prefer something more floral or something more woody?\n",
            "*   Are you drawn towards citrus notes or something more fresh?\n",
            "================================================================\n",
            "Answer Assistant Q3:\n",
            "Running prefill\n",
            "Running decode\n",
            "Okay! Let's talk about coffee and feeling tired – I understand you’re feeling drained sometimes. It’s definitely something many people experience! Let’s be honest – while coffee *can* be helpful *sometimes*, let’s be clear about *when* you need it.  I want you feeling your best and I want you energized!\n",
            "\n",
            "So you’re asking if coffee helps *when* you’re feeling so tired? That’s an excellent question! Let’s break down what’s going on:\n",
            "\n",
            "**Generally speaking...**\n",
            "\n",
            "*   **Coffee *can* provide some temporary relief.** When you’re tired you’re likely experiencing fatigue – your body isn’t functioning optimally. Caffeine stimulates your central nervous system – your brain and body – which can give you an initial boost! However...\n",
            "*   **It’s not *always* better.**  Excessive caffeine can lead you down an energy spiral!  It can cause anxiety and jitters which can be counterproductive.\n",
            "*   **There’s an optimal amount.**  What’s *right* for you depends on your individual tolerance and what you’re trying *to* achieve!\n",
            "\n",
            "**Let’s talk about *when* you *need* coffee:**\n",
            "\n",
            "*   **Before an intense workout:** A strong cup can provide an initial burst of energy and help you push harder.\n",
            "*   **When you’re feeling sluggish after sleep:** If you’ve had an irregular sleep schedule or have been struggling with sleep quality you may need some caffeine!\n",
            "*   **When you’re feeling overwhelmed or stressed:** A little bit can be helpful! A quick boost can provide some clarity and help calm your nerves.\n",
            "*   **When you’re feeling low on energy throughout your day.** A cup can provide an immediate pick-me-up!\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Listen To Your Body:** If you’re feeling *really* tired *and* caffeine isn’t helping or isn’t making you feel better – don’t reach for coffee!\n",
            "*   **Hydration:**  Drinking plenty of water *is* crucial! Dehydration can contribute significantly *to* fatigue and can be mistaken for tiredness!\n",
            "*   **Diet:** A balanced diet and regular exercise are *always* better than relying on caffeine!\n",
            "*   **Sleep:** Prioritize sleep! Aiming for at least 7-8 hours per night can make all difference!\n",
            "\n",
            "**I want you feeling your best!**  I can help you find strategies for improving your energy levels – whether that’s through better sleep habits and healthy eating and exercise! Do you want me take some time right now?\n",
            "================================================================\n",
            "Answer Assistant Q4:\n",
            "Running prefill\n",
            "Running decode\n",
            "Okay! Let’s talk about why citrus perfumes are so captivating – they’re an olfactory symphony! Here’s my breakdown:\n",
            "\n",
            "**First, let’s talk about what citrus *actually* smells like:**\n",
            "\n",
            "*   **It’s not just \"lemon\":** While lemon isn’t *always* at its core, many citrus perfumes are built around *layered* citrus notes – think grapefruit, lime, bergamot – they’re complex!\n",
            "*   **It’s about *brightness*:** Citrus perfumes are designed for an uplifting experience! They’re packed with aldehydes – molecules that give scents an airy, sparkling quality – which creates this feeling you get when smelling something bright and fresh.\n",
            "*   **It’s about *freshness*:** The bright acidity creates an immediate sensation – you’re immediately hit by an element of freshness and vitality!\n",
            "\n",
            "\n",
            "**Now let’s look at *why* they’re so appealing:**\n",
            "\n",
            "*   **Aromatic Compounds:** Citrus contains many aromatic compounds – esters and terpenes – which give it its unique scent profile! These compounds interact with our noses and our brains creating an experience!\n",
            "*   **Psychological Association:** Citrus has strong associations with positive feelings – happiness and energy! When we smell something bright and clean like citrus can trigger memories of sunshine and summer days!\n",
            "*   **Complexity:** A well-crafted citrus perfume isn’t just one note! It’s an *ensemble* – layering different citrus notes creates an intricate scent and an overall experience!\n",
            "*   **This is *so* versatile:** Citrus can be used as an accent note or as an base note! This versatility lets perfumes be tailored into various moods!\n",
            "\n",
            "\n",
            "**Ultimately:** A beautiful citrus perfume isn’t just about *one* scent; it’s about creating an *experience*! It’s about lifting your mood and giving you an instant feeling of joy! Do you want me give you some examples?\n",
            "================================================================\n",
            "Answer Assistant Q5:\n",
            "Running prefill\n",
            "Running decode\n",
            "Okay! Let’s find out where you can get Humburger! I’m really happy you’re interested – it’s really popular!\n",
            "\n",
            "I’m not an official retailer or distributor, so I can’t give you an exact address or website link *right now*. However, here’s what I can tell you:\n",
            "\n",
            "*   **It’s generally available at:**\n",
            "    *   **Amazon:** This’s usually your best bet – Amazon carries Humburger quite regularly!\n",
            "    *   **Other Online Retailers:** Check out stores like **Walmart** and **Target** – sometimes they carry Humburger or have it available for purchase online.\n",
            "    *   **Specialty Stores:** Occasionally you can find it at smaller specialty stores that sell unique or unusual items – these can be tricky but sometimes worth checking!\n",
            "\n",
            "*   **I can give you some tips:**\n",
            "    *   **Keep an eye out!** Humburger can sometimes be restocked or available at different retailers depending on demand.\n",
            "    *   **If you can’t find it easily:** Don’t worry! If you can’t find it at any of these places right now – you can always try searching for \"Humburger\" on Google or other search engines! You might find other similar items or be able find something similar!\n",
            "\n",
            "Do you want me give you some links or some more information?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "UpqjCyA-Vezr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "\n",
        "# Prepare dataset\n",
        "data = [\n",
        "    {\"prompt\": \"What's the best perfumes you have that highlight oud?\",\n",
        "     \"response\": \"Absolutely! Here are 10 exquisite perfumes featuring oud: Oud Royale, Midnight Velvet, Desert Whisper, Royal Cedar Oud, Black Mirage, Oud Blossom, Sandstorm Oud, Amethyst Woods, Eternal Night, and Mystic Oud Bloom.\"},\n",
        "\n",
        "    {\"prompt\": \"I'm craving something fresh and citrusy. What do you recommend?\",\n",
        "     \"response\": \"You’ll adore these refreshing picks: Citrus Grove, Amalfi Breeze, Lemon Muse, Golden Mandarin, Zesty Reverie, Sunrise Coast, Capri Gardens, Ocean Bloom, Sunlit Citrus, and Sparkling Lime Essence.\"},\n",
        "\n",
        "    {\"prompt\": \"Give me perfumes that feel romantic and floral.\",\n",
        "     \"response\": \"Love is in the air with these floral treasures: Rose Reverie, Jasmine Serenade, Blooming Dream, Garden Affair, Magnolia Muse, Peony Whisper, Petal Dusk, Lavender Mirage, Secret Garden, and Scarlet Blossom.\"},\n",
        "\n",
        "    {\"prompt\": \"I want something bold and smoky. What do you have?\",\n",
        "     \"response\": \"Boldness awaits you in these scents: Ember Legend, Black Smoke, Smoldering Nights, Ashen Oud, Tobacco Mystique, Charcoal Dream, Leather Inferno, Obsidian Spirit, Shadowfire, and Wild Smoke Trails.\"},\n",
        "\n",
        "    {\"prompt\": \"Recommend me perfumes that have vanilla notes.\",\n",
        "     \"response\": \"Sweet indulgence incoming! Try Vanilla Reverie, Golden Amber Vanilla, Dreamy Whispers, Midnight Vanilla, Silk Essence, Creamy Sandalwood, Amber Frost, Velvet Nectar, Whispered Woods, and Frosted Velvet.\"},\n",
        "\n",
        "    {\"prompt\": \"I’m searching for something green and earthy.\",\n",
        "     \"response\": \"Nature lovers rejoice with: Forest Bloom, Green Reverie, Moss Whisper, Woodland Echoes, Fresh Canopy, Earthborn Essence, Hidden Grove, Sage Mirage, Verdant Spirit, and Meadowlight.\"},\n",
        "\n",
        "    {\"prompt\": \"Show me your perfumes that are aquatic and fresh.\",\n",
        "     \"response\": \"Dive into these oceanic wonders: Aqua Myth, Deep Sea Whisper, Sapphire Tide, Coastal Dream, Ocean Drift, Crystal Waters, Sea Breeze Essence, Marine Mirage, Tidal Bloom, and Azure Horizons.\"},\n",
        "\n",
        "    {\"prompt\": \"I want something cozy and woody for winter.\",\n",
        "     \"response\": \"You’ll feel wrapped in warmth with: Winter Cedar, Fireside Muse, Whispered Pine, Ebony Woods, Chestnut Dream, Wooded Mirage, Smoky Oak, Hearthwood, Frosted Timber, and Darkwood Essence.\"},\n",
        "\n",
        "    {\"prompt\": \"Suggest perfumes with strong patchouli vibes.\",\n",
        "     \"response\": \"Patchouli power awaits in: Patchouli Mystique, Midnight Forest, Dark Reverie, Enchanted Soil, Patchouli Royale, Earthy Dreams, Mystic Patchouli, Black Earth, Patchouli Bloom, and Grounded Spirit.\"},\n",
        "\n",
        "    {\"prompt\": \"What perfumes feel luxurious and golden?\",\n",
        "     \"response\": \"Golden elegance is captured in: Aurum Essence, Sunlit Muse, Golden Horizon, Amberlight, Honeyed Mirage, Solar Reverie, Gleaming Woods, Radiant Sands, Bright Bloom, and Luxe Whisper.\"},\n",
        "\n",
        "    {\"prompt\": \"Give me perfumes that smell like a mysterious forest.\",\n",
        "     \"response\": \"Adventure calls with: Midnight Woods, Foggy Grove, Twilight Pine, Enchanted Canopy, Shadowed Ferns, Whispering Bark, Mystic Timber, Dusk Grove, Hidden Woods, and Sylvan Secrets.\"},\n",
        "\n",
        "    {\"prompt\": \"Recommend something daring and spicy.\",\n",
        "     \"response\": \"Set your senses ablaze with: Fiery Spice, Saffron Mirage, Scarlet Flame, Cinnamon Dream, Cardamom Spirit, Peppery Muse, Exotic Ember, Heatwave Essence, Spiced Reverie, and Volcanic Bloom.\"},\n",
        "\n",
        "    {\"prompt\": \"I want perfumes that smell expensive and elegant.\",\n",
        "     \"response\": \"Pure sophistication lives in: Opulent Bloom, Cashmere Dreams, Velvet Oud, Amber Royale, Supreme Whisper, Ivory Sandalwood, Pearl Blossom, Platinum Muse, Timeless Essence, and Celestial Bloom.\"},\n",
        "\n",
        "    {\"prompt\": \"What perfumes are best if I want something powdery and soft?\",\n",
        "     \"response\": \"For gentle elegance, explore: Powdered Dream, Soft Rose, Velvet Cloud, Lilac Whisper, Cotton Bloom, Dusty Petals, Featherlight Essence, Dreamy Magnolia, Cloudy Reverie, and Tender Blossom.\"},\n",
        "\n",
        "    {\"prompt\": \"Recommend perfumes that feel dark and sensual.\",\n",
        "     \"response\": \"Dive deep into: Black Velvet, Dark Orchid, Noir Mystique, Sultry Amber, Moonlit Bloom, Midnight Flame, Smoked Rose, Obsidian Nights, Dark Oud Mirage, and Crimson Eclipse.\"},\n",
        "\n",
        "    {\"prompt\": \"Show me perfumes perfect for a summer day.\",\n",
        "     \"response\": \"Sunny days deserve: Citrus Whisper, Sun Kissed Bloom, Tropical Muse, Summer Mirage, Lemon Breeze, Ocean Bloom, Island Dreams, Coral Coast, Peachy Reverie, and Sunset Glow.\"},\n",
        "\n",
        "    {\"prompt\": \"I want something comforting, like a cozy hug.\",\n",
        "     \"response\": \"Wrap yourself up with: Cashmere Cloud, Warm Vanilla Bloom, Sweet Sandalwood, Amber Embrace, Cozy Cedar, Whispered Amber, Hearth Glow, Frosted Petals, Cuddled Bloom, and Velvet Hearth.\"},\n",
        "\n",
        "    {\"prompt\": \"Recommend something sharp and energizing for mornings.\",\n",
        "     \"response\": \"Start your day with a spark: Electric Citrus, Lemon Zest Spirit, Morning Bloom, Crisp Lavender, Sparkling Mint, Awakening Rose, Fresh Horizon, Radiant Citrus, Dewdrop Essence, and Uplifting Sage.\"},\n",
        "\n",
        "    {\"prompt\": \"Give me perfumes that feel ancient and mystical.\",\n",
        "     \"response\": \"Unlock timeless secrets with: Mystic Oud, Ancient Amber, Saffron Spirit, Forgotten Grove, Eternal Sands, Sacred Smoke, Mythical Bloom, Enchanted Resin, Timeless Reverie, and Desert Mirage.\"},\n",
        "\n",
        "    {\"prompt\": \"I want something delicate but unforgettable.\",\n",
        "     \"response\": \"Subtle beauty shines in: Whispered Peony, Soft Magnolia, Dreamy Jasmine, Silken Rose, Lilac Muse, Dusk Petals, Feathered Bloom, Serenity Woods, Twilight Essence, and Delicate Dawn.\"},\n",
        "]\n",
        "\n",
        "# Create a Huggingface Dataset\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize_function(example):\n",
        "    input_text = f\"User: {example['prompt']}\\nAssistant: {example['response']}\"\n",
        "    tokenized = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
        "\n",
        "# Setup Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gemma-3-finetuned\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_checkpointing=True,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./gemma-3-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gemma-3-finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "9d9b8c6f18014aae83db9b60b854125e",
            "b14b46ed71df4f5381facf45667e948d",
            "2423935283e849f3a8b1eddae3b1420e",
            "b325b179aae144e9b1644f95fa33788f",
            "2c66f8d9b2d5480293d9a4279cac1a1b",
            "b6a93e3e012e4e10b44d9f568df632d7",
            "83e4ad35b2aa4ad7bdd7aaf5cd0b3f6f",
            "fb70121cf4034621959fa1c2765a0497",
            "a9f2f9de733744eeb834c7e8bef25ffc",
            "8aca7c0546554c3a90bca1e8d672a42b",
            "a9a573b2bb694a279023cebed7e47ce2"
          ]
        },
        "id": "mpVlSG8gIqka",
        "outputId": "f7e33a61-8724-4f19-c284-31833d7a4746"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d9b8c6f18014aae83db9b60b854125e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>22.224800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.334100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.173100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.201200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gemma-3-finetuned/tokenizer_config.json',\n",
              " './gemma-3-finetuned/special_tokens_map.json',\n",
              " './gemma-3-finetuned/tokenizer.model',\n",
              " './gemma-3-finetuned/added_tokens.json',\n",
              " './gemma-3-finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6Ny_9zfrIwCr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gemma-3-finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gemma-3-finetuned\")\n",
        "\n",
        "# Example prompt\n",
        "inputs = tokenizer(\"Can you suggest 5 luxury perfumes that are elegant and sophisticated?\", return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQolEMw9Ixcs",
        "outputId": "b679f3fc-fe4b-4841-abfc-205c5d2026f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you suggest 5 luxury perfumes that are elegant and sophisticated?\n",
            "Absolutely! Here are 5 elegant floral creations: Cashmere, Ivory Reverie, Supreme Bloom, Silk Essence, Velvet Cloud, Soft Magnolia, Platinum Muse, Serenade Whispers, Timeless Essence, and Luxe Whisper.\n",
            "Golden Elegance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "\n",
        "# Example prompt\n",
        "inputs = tokenizer(\"Can you suggest 5 luxury perfumes that are elegant and sophisticated?\", return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhOhp_L3IzJO",
        "outputId": "3a1f788e-0ba1-4942-b1c4-e5fe730e887d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you suggest 5 luxury perfumes that are elegant and sophisticated?\n",
            "\n",
            "Here are 5 recommendations that I think fit your description:\n",
            "\n",
            "1.  **Dior J'adore:** A classic, rosy floral perfume with notes of pink jasmine, rose, and a touch of amber. It's endlessly elegant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "\n",
        "# Example prompt\n",
        "inputs = tokenizer(\"Can you suggest 5 luxury perfumes that are elegant and sophisticated?\", return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ7x5t8jI0Mb",
        "outputId": "b795921e-4c79-4c25-affe-d253ff79edcb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you suggest 5 luxury perfumes that are elegant and sophisticated?\n",
            "\n",
            "Here are my suggestions:\n",
            "\n",
            "1.  **Chanel Coco Mademoiselle:**  A timeless classic, known for its bright orange top notes and a warm, fruity base. It's a wonderful, complex fragrance that is always popular.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load your fine-tuned Gemma model\n",
        "model = AutoModelForCausalLM.from_pretrained('./gemma-3-finetuned')\n",
        "tokenizer = AutoTokenizer.from_pretrained('./gemma-3-finetuned')\n",
        "\n",
        "# Set model to evaluation mode (important for inference and tracing)\n",
        "model.eval()\n",
        "\n",
        "# Create a wrapper to ONLY return logits (no past_key_values etc.)\n",
        "class GemmaLiteWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs.logits  # Only return logits, not the full CausalLMOutput\n",
        "\n",
        "# Wrap the model\n",
        "lite_model = GemmaLiteWrapper(model)\n"
      ],
      "metadata": {
        "id": "pkN6tsU-XF3l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dummy input for tracing\n",
        "# inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
        "\n",
        "# # Trace the model\n",
        "# traced_model = torch.jit.trace(lite_model, (inputs[\"input_ids\"], inputs[\"attention_mask\"]))\n",
        "\n",
        "# # Save the traced model\n",
        "# traced_model.save(\"gemma_lite_traced.pt\")"
      ],
      "metadata": {
        "id": "HXWG_4pCXImM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scripted_model = torch.jit.script(lite_model)\n",
        "\n",
        "# # Save the scripted model\n",
        "# scripted_model.save(\"gemma_lite_scripted.pt\")\n"
      ],
      "metadata": {
        "id": "-H0ZOr72XKqc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import ai_edge_torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# 1. Wrap the model\n",
        "class GemmaLiteWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        output = self.model(input_ids=input_ids, use_cache=False)\n",
        "        return output.logits\n",
        "\n",
        "# 2. Load fine-tuned Gemma\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gemma-3-finetuned\").eval()\n",
        "wrapped_model = GemmaLiteWrapper(model)\n",
        "print(\"Defining wrapped model....\")\n",
        "\n",
        "# 3. Dummy input\n",
        "sample_input = (torch.randint(0, model.config.vocab_size, (1, 5)),)\n",
        "\n",
        "print(\"generating dummy input....\")\n",
        "# 4. Convert (basic, without optimize/quantize for now)\n",
        "edge_model = ai_edge_torch.convert(\n",
        "    wrapped_model,\n",
        "    sample_input,\n",
        ")\n",
        "\n",
        "# 5. Export to TFLite (still full precision)\n",
        "edge_model.export('gemma_lite_full.tflite')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "n1nvBQj4XM6i",
        "outputId": "d9d1c321-4294-4c80-fad4-f3e3e9863c09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:354: UserWarning: Device capability of jax unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
            "  warnings.warn(\n",
            "WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-15f43a07ae03>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 4. Convert (basic, without optimize/quantize for now)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m edge_model = ai_edge_torch.convert(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mwrapped_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msample_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ai_edge_torch/_convert/converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(module, sample_args, sample_kwargs, strict_export, quant_config, dynamic_shapes, _ai_edge_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0m_ai_edge_converter_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m   return Converter().convert(\n\u001b[0m\u001b[1;32m    262\u001b[0m       \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ai_edge_torch/_convert/converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, module, sample_args, sample_kwargs, strict_export, quant_config, dynamic_shapes, _ai_edge_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;34m\" specified.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         )\n\u001b[0;32m--> 172\u001b[0;31m     return conversion.convert_signatures(\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mstrict_export\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict_export\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ai_edge_torch/_convert/conversion.py\u001b[0m in \u001b[0;36mconvert_signatures\u001b[0;34m(signatures, strict_export, quant_config, _tfl_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;31m# Apply default fx passes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0mexported_programs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_convert_passes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexported_programs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   tflite_model = lowertools.exported_programs_to_tflite(\n\u001b[0m\u001b[1;32m    151\u001b[0m       \u001b[0mexported_programs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ai_edge_torch/lowertools/_shim.py\u001b[0m in \u001b[0;36mexported_programs_to_tflite\u001b[0;34m(exported_programs, signatures, quant_config, _tfl_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m     78\u001b[0m   )\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m   return utils.merged_bundle_to_tfl_model(\n\u001b[0m\u001b[1;32m     81\u001b[0m       \u001b[0mmerged_bundle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ai_edge_torch/lowertools/odml_torch_utils.py\u001b[0m in \u001b[0;36mmerged_bundle_to_tfl_model\u001b[0;34m(merged_bundle, signatures, quant_config, _tfl_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mconversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_tfl_converter_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_tfl_converter_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1571\u001b[0m       )\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m     \u001b[0mtrackable_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrackable_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m       self._debug_info = _get_debug_info(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         loader = Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m   1043\u001b[0m                         ckpt_options, options, filters)\n\u001b[1;32m   1044\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_skip_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturableResource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1501\u001b[0m         saveables_cache=self._saveables_cache)\n\u001b[1;32m   1502\u001b[0m     restore_lib.CheckpointPosition(\n\u001b[0;32m-> 1503\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1504\u001b[0m                                                    reader)\n\u001b[1;32m   1505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/restore.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable, reader)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_descendants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/restore.py\u001b[0m in \u001b[0;36m_restore_descendants\u001b[0;34m(self, reader)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     restore_ops.extend(\n\u001b[0;32m--> 533\u001b[0;31m         current_position.checkpoint.restore_saveables(\n\u001b[0m\u001b[1;32m    534\u001b[0m             \u001b[0mtensor_saveables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpython_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregistered_savers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_positions, registered_savers, reader)\u001b[0m\n\u001b[1;32m    382\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver.from_saveables(\n\u001b[1;32m    383\u001b[0m           \u001b[0mflat_saveables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m           registered_savers).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[1;32m    385\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    624\u001b[0m                   restored_tensors[trackable_utils.extract_local_name(\n\u001b[1;32m    625\u001b[0m                       ckpt_key)] = tensor\n\u001b[0;32m--> 626\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                   \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36m_restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    745\u001b[0m                        f\"\\n\\tGot: {list(restored_tensors.keys())}\")\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msaveable_object_to_restore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36m_restore_from_tensors\u001b[0;34m(restored_tensors)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0msaveable_restored_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice_spec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m       restore_ops[saveable.name] = saveable.restore(\n\u001b[0m\u001b[1;32m    785\u001b[0m           saveable_restored_tensors, restored_shapes=None)\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_mapped_captures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrestored_tensor_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;34m\"\"\"Implements Trackable._restore_from_tensors.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m       restored_tensor = array_ops.identity(\n\u001b[0m\u001b[1;32m    804\u001b[0m           restored_tensors[trackable.VARIABLE_VALUE_KEY])\n\u001b[1;32m    805\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/gemma_lite_full.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/gemma-3-finetuned\")\n",
        "\n",
        "# Prepare input text\n",
        "text = \"Hello how are you\"\n",
        "\n",
        "# Get the expected shape for input\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Expected input shape [1, seq_len] (e.g., [1, 5])\n",
        "expected_input_shape = input_details[0]['shape']\n",
        "seq_len = expected_input_shape[1]  # Get the expected sequence length\n",
        "\n",
        "# Tokenize the input text and ensure it fits the expected sequence length\n",
        "inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    padding=\"max_length\",  # Pad to max length if needed\n",
        "    truncation=True,       # Truncate if the length exceeds max_length\n",
        "    max_length=seq_len    # Ensure that the sequence length matches\n",
        ")\n",
        "\n",
        "# Ensure that the shape of the input matches the expected shape [1, seq_len]\n",
        "input_ids = inputs['input_ids']\n",
        "\n",
        "# Start with the input sequence and predict a continuation\n",
        "generated_sequence = input_ids[0].tolist()  # Start with the input sequence as the base\n",
        "num_tokens_to_generate = 20  # Adjust the number of tokens to generate\n",
        "\n",
        "for _ in range(num_tokens_to_generate):\n",
        "    # Set the input tensor to the interpreter\n",
        "    # interpreter.set_tensor(input_details[0]['index'], np.array([generated_sequence]))\n",
        "    # Take only the last `seq_len` tokens\n",
        "    input_for_inference = np.array([generated_sequence[-seq_len:]])\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_for_inference)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output tensor\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Extract the predicted token (take the last token prediction)\n",
        "    predicted_token_id = np.argmax(output[0, -1])  # Take the last token for prediction\n",
        "\n",
        "    # Add the predicted token to the generated sequence\n",
        "    generated_sequence.append(predicted_token_id)\n",
        "\n",
        "# Decode the generated sequence\n",
        "generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nGenerated Text:\", generated_text)\n"
      ],
      "metadata": {
        "id": "r2kHKeFSYPAA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}